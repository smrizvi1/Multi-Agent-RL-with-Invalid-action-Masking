{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb85dc4-dc23-4811-bf07-e9dfb4b0241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from .action_masking import ActionMasking\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, num_uavs, num_users):\n",
    "        self.state_dim = state_dim\n",
    "        self.num_uavs = num_uavs\n",
    "        self.num_users = num_users\n",
    "        self.action_dim = 126  # Total action space size\n",
    "        \n",
    "        # Network parameters\n",
    "        self.update_freq = 2000\n",
    "        self.replay_size = 20000\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # Initialize replay memory\n",
    "        self.replay_buffer = deque(maxlen=self.replay_size)\n",
    "        self.step = 0\n",
    "        \n",
    "        # Initialize action masking\n",
    "        self.action_masker = ActionMasking(self.action_dim)\n",
    "        \n",
    "        # Create networks\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build neural network with integrated action masking\"\"\"\n",
    "        # State input\n",
    "        state_input = Input(shape=(self.state_dim,))\n",
    "        \n",
    "        # Action mask input\n",
    "        mask_input = Input(shape=(self.action_dim,))\n",
    "        \n",
    "        # Hidden layers\n",
    "        x = Dense(90, activation='relu')(state_input)\n",
    "        x = Dense(90, activation='relu')(x)\n",
    "        \n",
    "        # Q-value output before masking\n",
    "        q_values = Dense(self.action_dim)(x)\n",
    "        \n",
    "        # Apply action mask\n",
    "        masked_q_values = self.action_masker.masking_layer([q_values, mask_input])\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(inputs=[state_input, mask_input], outputs=masked_q_values)\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), \n",
    "                     loss='mse')\n",
    "        return model\n",
    "\n",
    "    def state_abstraction(self, state, serving_uav):\n",
    "        \"\"\"Create abstracted state representation as per paper\"\"\"\n",
    "        # Reorder UAV positions to put serving UAV first\n",
    "        uav_positions = state[:self.num_uavs * 3].reshape(-1, 3)\n",
    "        serving_uav_pos = uav_positions[serving_uav].copy()\n",
    "        uav_positions[0], uav_positions[serving_uav] = serving_uav_pos, uav_positions[0]\n",
    "        \n",
    "        # Reorder channel gains based on serving UAV\n",
    "        channel_gains = state[self.num_uavs * 3:].copy()\n",
    "        serving_users = np.where(state[-self.num_users:] == serving_uav)[0]\n",
    "        for i, user in enumerate(serving_users):\n",
    "            channel_gains[i], channel_gains[user] = channel_gains[user], channel_gains[i]\n",
    "            \n",
    "        return np.concatenate([uav_positions.flatten(), channel_gains])\n",
    "\n",
    "    def choose_action(self, state, epsilon, serving_uav, user_association):\n",
    "        \"\"\"Choose action using epsilon-greedy policy with action masking\"\"\"\n",
    "        cluster_size = len(np.where(user_association.iloc[0,:] == serving_uav)[0])\n",
    "        mask = self.action_masker.get_mask(cluster_size)\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            return self.action_masker.get_random_valid_action(cluster_size)\n",
    "        \n",
    "        # Get Q-values with masking\n",
    "        state = np.expand_dims(self.state_abstraction(state, serving_uav), axis=0)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        q_values = self.model.predict([state, mask], verbose=0)[0]\n",
    "        \n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def remember(self, state, action, next_state, reward, serving_uav, cluster_size):\n",
    "        \"\"\"Store experience in replay buffer with abstracted states\"\"\"\n",
    "        abstracted_state = self.state_abstraction(state, serving_uav)\n",
    "        abstracted_next_state = self.state_abstraction(next_state, serving_uav)\n",
    "        self.replay_buffer.append((abstracted_state, action, abstracted_next_state, \n",
    "                                 reward, cluster_size))\n",
    "\n",
    "    def train(self, gamma=0.99):\n",
    "        \"\"\"Train the network using experience replay and action masking\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.step += 1\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states = np.array([exp[0] for exp in batch])\n",
    "        actions = np.array([exp[1] for exp in batch])\n",
    "        next_states = np.array([exp[2] for exp in batch])\n",
    "        rewards = np.array([exp[3] for exp in batch])\n",
    "        cluster_sizes = np.array([exp[4] for exp in batch])\n",
    "        \n",
    "        # Generate masks for current and next states\n",
    "        masks = np.array([self.action_masker.get_mask(size)[0] for size in cluster_sizes])\n",
    "        next_masks = masks.copy()\n",
    "        \n",
    "        # Get current Q values\n",
    "        current_q = self.model.predict([states, masks], verbose=0)\n",
    "        \n",
    "        # Get next Q values from target network\n",
    "        next_q = self.target_model.predict([next_states, next_masks], verbose=0)\n",
    "        \n",
    "        # Update Q values\n",
    "        for i in range(self.batch_size):\n",
    "            current_q[i][actions[i]] = rewards[i] + gamma * np.max(next_q[i])\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit([states, masks], current_q, batch_size=self.batch_size, \n",
    "                      verbose=0)\n",
    "        \n",
    "        # Update target network\n",
    "        if self.step % self.update_freq == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "    def get_power_allocation(self, action, cluster_size):\n",
    "        \"\"\"Get power allocation scheme from action\"\"\"\n",
    "        return self.action_masker.get_power_allocation_scheme(action, cluster_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
