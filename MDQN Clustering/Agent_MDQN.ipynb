{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211c8d0-4253-4b3c-bc28-45b1e44562e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from config import *\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.update_freq = UPDATE_FREQ\n",
    "        self.replay_size = REPLAY_SIZE\n",
    "        self.step = 0\n",
    "        self.replay_queue = deque(maxlen=self.replay_size)\n",
    "        \n",
    "        # Calculate action space\n",
    "        self.power_number = 3 ** (NumberOfUsers / NumberOfUAVs)\n",
    "        self.action_number = 7 * int(self.power_number)\n",
    "        \n",
    "        # Create models\n",
    "        self.model = self._create_model()\n",
    "        self.target_model = self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create the DQN model architecture\"\"\"\n",
    "        STATE_DIM = NumberOfUAVs * 3 + NumberOfUsers\n",
    "        ACTION_DIM = 7 * self.power_number\n",
    "        \n",
    "        model = models.Sequential([\n",
    "            layers.Dense(40, input_dim=STATE_DIM, activation='relu'),\n",
    "            layers.Dense(ACTION_DIM, activation=\"linear\")\n",
    "        ])\n",
    "        model.compile(\n",
    "            loss='mean_squared_error', \n",
    "            optimizer=optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.uniform() < epsilon:\n",
    "            return np.random.choice(self.action_number)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def remember(self, state, action, next_state, reward):\n",
    "        \"\"\"Store experience in replay memory\"\"\"\n",
    "        self.replay_queue.append((state, action, next_state, reward))\n",
    "\n",
    "    def train(self, batch_size=BATCH_SIZE, gamma=GAMMA):\n",
    "        \"\"\"Train the DQN using experience replay\"\"\"\n",
    "        if len(self.replay_queue) < self.replay_size:\n",
    "            return\n",
    "            \n",
    "        self.step += 1\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if self.step % self.update_freq == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "        # Sample batch from replay memory\n",
    "        batch = random.sample(self.replay_queue, batch_size)\n",
    "        states = np.array([x[0] for x in batch])\n",
    "        next_states = np.array([x[2] for x in batch])\n",
    "        \n",
    "        # Get Q-values for current and next states\n",
    "        current_q = self.model.predict(states)\n",
    "        next_q = self.target_model.predict(next_states)\n",
    "        \n",
    "        # Update Q-values using Bellman equation\n",
    "        for i, (_, action, _, reward) in enumerate(batch):\n",
    "            current_q[i][action] = reward + gamma * np.max(next_q[i])\n",
    "            \n",
    "        # Train the model\n",
    "        self.model.fit(states, current_q, verbose=0)\n",
    "        \n",
    "    def take_action_NOMA(self, acting_UAV, User_asso_list, ChannelGain_list):\n",
    "        \"\"\"Implement NOMA power allocation\"\"\"\n",
    "        acting_user_list = np.where(User_asso_list.iloc[0,:] == acting_UAV)[0]\n",
    "        First_user = acting_user_list[0]\n",
    "        Second_user = acting_user_list[1]\n",
    "        \n",
    "        first_user_CG = ChannelGain_list.iloc[0,First_user]\n",
    "        second_user_CG = ChannelGain_list.iloc[0,Second_user]\n",
    "        \n",
    "        # NOMA power allocation based on channel conditions\n",
    "        if first_user_CG >= second_user_CG:\n",
    "            User0 = Second_user  # Far user (higher power)\n",
    "            User1 = First_user   # Near user (lower power)\n",
    "        else:\n",
    "            User0 = First_user\n",
    "            User1 = Second_user\n",
    "            \n",
    "        self.Power_allocation_list.iloc[0,User0] = self.Power_unit * 3/4\n",
    "        self.Power_allocation_list.iloc[0,User1] = self.Power_unit * 1/4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
